{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-16T22:45:37.613473Z",
     "iopub.status.busy": "2025-05-16T22:45:37.613255Z",
     "iopub.status.idle": "2025-05-16T22:46:58.307106Z",
     "shell.execute_reply": "2025-05-16T22:46:58.306426Z",
     "shell.execute_reply.started": "2025-05-16T22:45:37.613455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# 0) install the BERTScore metric\n",
    "!pip --quiet install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:46:58.308206Z",
     "iopub.status.busy": "2025-05-16T22:46:58.308010Z",
     "iopub.status.idle": "2025-05-16T22:47:24.768166Z",
     "shell.execute_reply": "2025-05-16T22:47:24.767625Z",
     "shell.execute_reply.started": "2025-05-16T22:46:58.308184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 22:47:09.619493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747435629.833932      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747435629.896630      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from bert_score import score\n",
    "from peft import PeftModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:47:24.770111Z",
     "iopub.status.busy": "2025-05-16T22:47:24.769698Z",
     "iopub.status.idle": "2025-05-16T22:47:24.793424Z",
     "shell.execute_reply": "2025-05-16T22:47:24.792946Z",
     "shell.execute_reply.started": "2025-05-16T22:47:24.770094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) load your QA dataset\n",
    "df = pd.read_csv(\"/kaggle/input/test-data-curation/qa_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:47:24.794227Z",
     "iopub.status.busy": "2025-05-16T22:47:24.793982Z",
     "iopub.status.idle": "2025-05-16T22:47:24.893837Z",
     "shell.execute_reply": "2025-05-16T22:47:24.893362Z",
     "shell.execute_reply.started": "2025-05-16T22:47:24.794201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:47:24.894649Z",
     "iopub.status.busy": "2025-05-16T22:47:24.894447Z",
     "iopub.status.idle": "2025-05-16T22:47:24.897979Z",
     "shell.execute_reply": "2025-05-16T22:47:24.897230Z",
     "shell.execute_reply.started": "2025-05-16T22:47:24.894633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:47:24.899131Z",
     "iopub.status.busy": "2025-05-16T22:47:24.898694Z",
     "iopub.status.idle": "2025-05-16T22:48:23.426154Z",
     "shell.execute_reply": "2025-05-16T22:48:23.425601Z",
     "shell.execute_reply.started": "2025-05-16T22:47:24.899107Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93398c4930f43b2a7445cd6becd4b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28044019aae342579dbbd4970c6a2278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4c73c3cfc04d6bbe17c8a24aa33240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fe7c4d1ec7457dbdd0563d9354f2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10136f390614e63917b93fc7d564a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d682a5c71a884c738681103d8e18ab75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992833638e2c4108bf3c3d9a836aff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80dbca634744f8f8399cbb7fb78d456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3094917961d6472ab8714d741a08b745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21adb52051b347ec8d3f046a37b2a7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506b5d33443c4e98ac5c69833787bb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96579a002f424fcc9d76f84d6562467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da12d34aa61f4f35aa322afeff16a575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2202f0bbc7b48b5b70dc537ddbe7019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) Load the base PaliGemma-3B-pt-224 in FP16 (or full-precision if you prefer)\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    \"google/paligemma-3b-mix-224\",   # or your local base folder\n",
    "    torch_dtype=torch.float16,           # → load in half-precision\n",
    "    device_map=\"auto\",                   # → shard across your GPUs/TPUs\n",
    "    low_cpu_mem_usage=True               # → speed up init + lower host memory\n",
    ")\n",
    "\n",
    "# 2) Plug in your fine-tuned LoRA adapter\n",
    "lora_dir = \"/kaggle/input/16-may-paligemma-mix/finetuned_paligemma_mix/lora_adapters\"\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_dir,\n",
    "    torch_dtype=torch.float16,           # ensure the adapter is also in half-precision\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 3) Reload the processor (image+text) so it matches your base model’s preproc\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"google/paligemma-3b-mix-224\",   # same ID or local folder as in step 1\n",
    "    local_files_only=False              # remove if you want strictly offline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T22:48:23.427127Z",
     "iopub.status.busy": "2025-05-16T22:48:23.426866Z",
     "iopub.status.idle": "2025-05-16T23:10:32.046901Z",
     "shell.execute_reply": "2025-05-16T23:10:32.046092Z",
     "shell.execute_reply.started": "2025-05-16T22:48:23.427103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6732dd4e2f4caf8af12299d12ee000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PaliGemma Inference:   0%|          | 0/5994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 4) inference loop with progress bar\n",
    "preds, refs = [], []\n",
    "image_root  = \"/kaggle/input/test-data-dataset/images\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"PaliGemma Inference\"):\n",
    "    # load & preprocess image\n",
    "    img_path = os.path.join(image_root, row[\"path\"])\n",
    "    image    = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "     # build the prompted question with the required <image> token\n",
    "    question    = row[\"question\"]\n",
    "    # 1 <image> token because you have 1 image\n",
    "    prompt_text = f\"<image> Answer in exactly one word: {question}\"\n",
    "    \n",
    "    # tokenize image + prompt\n",
    "    inputs    = processor(text=prompt_text, images=image, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    # generate (greedy)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "    \n",
    "    # decode only the answer portion\n",
    "    answer_ids = output_ids[0, input_len:]\n",
    "    pred       = processor.decode(answer_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    preds.append(pred)\n",
    "    refs .append(str(row[\"answer\"]).strip())\n",
    "\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T23:10:32.048106Z",
     "iopub.status.busy": "2025-05-16T23:10:32.047821Z",
     "iopub.status.idle": "2025-05-16T23:10:54.088613Z",
     "shell.execute_reply": "2025-05-16T23:10:54.087828Z",
     "shell.execute_reply.started": "2025-05-16T23:10:32.048079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00286e02bea24a0b9c8e28f6f9e8dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b372d1b99354cd18ff1d5c2eec02894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b71d1e67394c6789398933cd989db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b519563032848da88c29327318d6fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f24c35ccb6420e8f1a377557dcaf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989de6f9c62f460dab75c16ed4f56779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4) compute metrics\n",
    "total_time    = end_time - start_time\n",
    "avg_time_ms   = total_time / len(preds) * 1000\n",
    "\n",
    "# Exact‐Match Accuracy (0–1)\n",
    "exact_match = sum(p.lower()==r.lower() for p,r in zip(preds, refs)) / len(preds)\n",
    "\n",
    "# BERTScore F1 for all predictions\n",
    "_, _, F1_all = score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "bert_f1_all  = F1_all.mean().item()\n",
    "\n",
    "# One‐word compliance & BERTScore on one‐word preds\n",
    "is_one_word = [((\" \" not in p) and p != \"\") for p in preds]\n",
    "pct_one_word = sum(is_one_word) / len(preds) * 100\n",
    "\n",
    "preds_1w = [p for p, ok in zip(preds, is_one_word) if ok]\n",
    "refs_1w  = [r for r, ok in zip(refs,  is_one_word) if ok]\n",
    "\n",
    "if preds_1w:\n",
    "    _, _, F1_1w = score(preds_1w, refs_1w, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_1w  = F1_1w.mean().item()\n",
    "else:\n",
    "    bert_f1_1w = float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T23:10:54.091019Z",
     "iopub.status.busy": "2025-05-16T23:10:54.090796Z",
     "iopub.status.idle": "2025-05-16T23:10:54.095686Z",
     "shell.execute_reply": "2025-05-16T23:10:54.094890Z",
     "shell.execute_reply.started": "2025-05-16T23:10:54.091002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time (1000 samples): 1328.6s\n",
      "Average per sample:                 221.7ms\n",
      "\n",
      "Exact-Match Accuracy:               0.78\n",
      "BERTScore F1 (all preds):           0.91\n",
      "\n",
      "% One-Word Predictions:             99.93%\n",
      "BERTScore F1 (one-word only):       0.91\n"
     ]
    }
   ],
   "source": [
    "# 5) report\n",
    "print(f\"Total inference time (1000 samples): {total_time:.1f}s\")\n",
    "print(f\"Average per sample:                 {avg_time_ms:.1f}ms\\n\")\n",
    "print(f\"Exact-Match Accuracy:               {exact_match:.2f}\")\n",
    "print(f\"BERTScore F1 (all preds):           {bert_f1_all:.2f}\\n\")\n",
    "print(f\"% One-Word Predictions:             {pct_one_word:.2f}%\")\n",
    "print(f\"BERTScore F1 (one-word only):       {bert_f1_1w:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7418943,
     "sourceId": 11812220,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 239697920,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 240061586,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
