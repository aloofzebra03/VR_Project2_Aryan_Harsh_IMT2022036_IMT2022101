{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11812220,"sourceType":"datasetVersion","datasetId":7418943},{"sourceId":239827451,"sourceType":"kernelVersion"},{"sourceId":239697920,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0) install BERTScore\n!pip --quiet install bert-score\n!pip --quiet install qwen-vl-utils\n!pip --quiet install -U bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:29:25.207414Z","iopub.execute_input":"2025-05-17T21:29:25.207608Z","iopub.status.idle":"2025-05-17T21:30:50.728613Z","shell.execute_reply.started":"2025-05-17T21:29:25.207591Z","shell.execute_reply":"2025-05-17T21:30:50.727668Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom peft import prepare_model_for_kbit_training, get_peft_model, peft_model\nfrom bert_score import score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:30:50.730160Z","iopub.execute_input":"2025-05-17T21:30:50.730448Z","iopub.status.idle":"2025-05-17T21:31:16.221325Z","shell.execute_reply.started":"2025-05-17T21:30:50.730425Z","shell.execute_reply":"2025-05-17T21:31:16.220755Z"}},"outputs":[{"name":"stderr","text":"2025-05-17 21:31:03.893259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747517464.133191      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747517464.194991      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1) loading QA dataset\ndf = pd.read_csv(\"/kaggle/input/test-data-curation/qa_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:31:16.222070Z","iopub.execute_input":"2025-05-17T21:31:16.222688Z","iopub.status.idle":"2025-05-17T21:31:16.256131Z","shell.execute_reply.started":"2025-05-17T21:31:16.222662Z","shell.execute_reply":"2025-05-17T21:31:16.255504Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 2) device setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:31:16.256815Z","iopub.execute_input":"2025-05-17T21:31:16.257015Z","iopub.status.idle":"2025-05-17T21:31:16.752783Z","shell.execute_reply.started":"2025-05-17T21:31:16.256999Z","shell.execute_reply":"2025-05-17T21:31:16.752011Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, Qwen2_5_VLProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom peft import PeftModel\nimport torch\n\n# 1) keep your existing processor\nprocessor = Qwen2_5_VLProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\", trust_remote_code=True,\n    min_pixels=224 * 224, #Since abo images in 256,we tried to keep a little lower than that.\n    max_pixels=1280*28*28,\n    \n)\n\n# 2) bits-and-bytes 4-bit config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# 3) load the base model in 4-bit\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True,\n)\n\n# 4) plug in your finetuned LoRA adapter and immediately merge\nmodel_ft = PeftModel.from_pretrained(\n    model,\n    \"/kaggle/input/vr2-qwen2-5-finetuned/lora_adapters_qwen_2_5_4b\",\n    device_map=\"auto\",\n)\nmodel_ft = model_ft.merge_and_unload()\n\n\nmodel_ft.eval()\nmodel_ft.config.use_cache = True\n\n# 6) adjust your tokenizer settings (processor.tokenizer is fine)\ntokenizer = processor.tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:31:16.754271Z","iopub.execute_input":"2025-05-17T21:31:16.754567Z","iopub.status.idle":"2025-05-17T21:32:39.995344Z","shell.execute_reply.started":"2025-05-17T21:31:16.754543Z","shell.execute_reply":"2025-05-17T21:32:39.994567Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011b48537f564738af3ac89f62ec84b6"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7b525878c24bebb72b49e4325e8e91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a60ac8ec194ee78b5d983803912e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ec91d69c4f4cbea09e45948004ba98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac6fa6d66a44e6688aebfbf0936a60d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8915d1de814e3e97bcdc7e225108ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a761992035cd47b9b47e7a0ad41b5044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df50aec66264b849f7d61f4e8e759de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484d76d0d3f54fb1a2d567099ea59dc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e3377919f8540a4b1baac1f85f59c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75235f88dc343d4853985127d2d6c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fe3f2429eb4e6a866fffd6ca3fabd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e997b4a0c7486c931c97657d5d1e64"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:396: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n```python\nfrom transformers import AutoModelForCausalLM\n\n# Load original tied model\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n\n# Set the randomly initialized lm_head to the previously tied embeddings\nmodel.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n\n# Save the untied model\nuntied_model_dir = \"dir/for/untied/model\"\nmodel.save_pretrained(untied_model_dir)\nmodel.config.save_pretrained(untied_model_dir)\n\n# Now use the original model but in untied format\nmodel = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n```\n\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import math\n\n# 1) parameters\nbatch_size  = 16\npreds, refs = [], []\nimage_root  = \"/kaggle/input/test-data-dataset/images\"\nn_samples   = len(df)\nn_batches   = math.ceil(n_samples / batch_size)\n\n# 2) batched inference\nstart_time = time.time()\nfor batch_start in tqdm(range(0, n_samples, batch_size),\n                        total=n_batches,\n                        desc=\"Qwen2.5-VL-3B-Instruct Batched Inference\"):\n    batch_df = df.iloc[batch_start : batch_start + batch_size]\n\n    # 2a) build list of “chat” messages for the batch\n    batch_messages = []\n    for _, row in batch_df.iterrows():\n        img = Image.open(os.path.join(image_root, row[\"path\"])).convert(\"RGB\").resize((256, 256))\n        batch_messages.append([\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": img},\n                {\"type\": \"text\",  \"text\": f\"Answer in exactly one word: {row['question']}\"}\n            ]}\n        ])\n\n    # 2b) apply the chat template in batch\n    batch_texts = [\n        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n        for msg in batch_messages\n    ]\n\n    # 2c) extract vision inputs for the whole batch\n    image_inputs, video_inputs = process_vision_info(batch_messages)\n\n    # 2d) tokenize the entire batch at once\n    inputs = processor(\n        text=batch_texts,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    # 2e) generate in one shot\n    with torch.inference_mode():\n        generated = model_ft.generate(**inputs, max_new_tokens=32)\n\n    # 2f) decode each example, trimming off the prompt\n    for in_ids, out_ids in zip(inputs.input_ids, generated):\n        trimmed_ids = out_ids[len(in_ids):]\n        text_out   = processor.batch_decode(\n            [trimmed_ids],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=False\n        )[0].strip()\n        preds.append(text_out)\n    # 2g) collect references\n    refs.extend(batch_df[\"answer\"].astype(str).str.strip().tolist())\n\nend_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:33:50.143152Z","iopub.execute_input":"2025-05-17T21:33:50.143687Z","iopub.status.idle":"2025-05-17T22:06:13.967668Z","shell.execute_reply.started":"2025-05-17T21:33:50.143662Z","shell.execute_reply":"2025-05-17T22:06:13.966796Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Qwen2.5-VL-3B-Instruct Batched Inference:   0%|          | 0/375 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b5139d514642708da504109619916e"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 5) compute metrics\ntotal_time    = end_time - start_time\navg_time_ms   = total_time / len(preds) * 1000\n\n# Exact‐Match Accuracy\nexact_match = sum(p.lower()==r.lower() for p,r in zip(preds, refs)) / len(preds)\n\n# BERTScore F1 (all preds)\nP_all, R_all, F1_all = score(preds, refs, lang=\"en\", rescale_with_baseline=True)\nbert_f1_all = F1_all.mean().item()\n\n# One‐word analysis\nis_one_word  = [((\" \" not in p) and p!= \"\") for p in preds]\npct_one_word = sum(is_one_word) / len(preds) * 100\npreds_1w     = [p for p, ok in zip(preds, is_one_word) if ok]\nrefs_1w      = [r for r, ok in zip(refs,  is_one_word) if ok]\n\nif preds_1w:\n    _, _, F1_1w = score(preds_1w, refs_1w, lang=\"en\", rescale_with_baseline=True)\n    bert_f1_1w = F1_1w.mean().item()\nelse:\n    bert_f1_1w = float(\"nan\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T22:06:13.969067Z","iopub.execute_input":"2025-05-17T22:06:13.969292Z","iopub.status.idle":"2025-05-17T22:06:33.335874Z","shell.execute_reply.started":"2025-05-17T22:06:13.969275Z","shell.execute_reply":"2025-05-17T22:06:33.334839Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d560e033784c83aac4f583554917f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305adf18f4ee401eaf41dc9c459c06fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9693baf65754dfd945809477c35d65a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc4a0e13a374125840e8fd74560cf3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68255305fbd4ff7bf1b66b240275b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c284e5a4a20142c38655845d98a683cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 6) report\nprint(f\"Total inference time (1000 samples): {total_time:.1f}s\")\nprint(f\"Average per sample:                {avg_time_ms:.1f}ms\\n\")\nprint(f\"Exact-Match Accuracy:              {exact_match:.2f}\")\nprint(f\"BERTScore F1 (all preds):          {bert_f1_all:.2f}\\n\")\nprint(f\"% One-Word Predictions:            {pct_one_word:.2f}%\")\nprint(f\"BERTScore F1 (one-word only):      {bert_f1_1w:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T22:06:33.336896Z","iopub.execute_input":"2025-05-17T22:06:33.337139Z","iopub.status.idle":"2025-05-17T22:06:33.342626Z","shell.execute_reply.started":"2025-05-17T22:06:33.337122Z","shell.execute_reply":"2025-05-17T22:06:33.341694Z"}},"outputs":[{"name":"stdout","text":"Total inference time (1000 samples): 1943.8s\nAverage per sample:                324.3ms\n\nExact-Match Accuracy:              0.27\nBERTScore F1 (all preds):          0.71\n\n% One-Word Predictions:            93.93%\nBERTScore F1 (one-word only):      0.75\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}