{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11812220,"sourceType":"datasetVersion","datasetId":7418943},{"sourceId":11850493,"sourceType":"datasetVersion","datasetId":7446201},{"sourceId":239697920,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0) install BERTScore\n!pip --quiet install bert-score\n!pip --quiet install qwen-vl-utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:49:08.931409Z","iopub.execute_input":"2025-05-17T18:49:08.931627Z","iopub.status.idle":"2025-05-17T18:50:32.382099Z","shell.execute_reply.started":"2025-05-17T18:49:08.931608Z","shell.execute_reply":"2025-05-17T18:50:32.381137Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\nimport torch\nfrom PIL import Image, ImageOps\nfrom tqdm.auto import tqdm\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer\nfrom bert_score import score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:50:32.384647Z","iopub.execute_input":"2025-05-17T18:50:32.384919Z","iopub.status.idle":"2025-05-17T18:50:59.211277Z","shell.execute_reply.started":"2025-05-17T18:50:32.384895Z","shell.execute_reply":"2025-05-17T18:50:59.210449Z"}},"outputs":[{"name":"stderr","text":"2025-05-17 18:50:45.869536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747507846.072038      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747507846.125734      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1) loading QA dataset\ndf = pd.read_csv(\"/kaggle/input/test-data-curation/qa_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:50:59.212180Z","iopub.execute_input":"2025-05-17T18:50:59.212821Z","iopub.status.idle":"2025-05-17T18:50:59.245347Z","shell.execute_reply.started":"2025-05-17T18:50:59.212798Z","shell.execute_reply":"2025-05-17T18:50:59.244666Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 2) device setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:50:59.246170Z","iopub.execute_input":"2025-05-17T18:50:59.246386Z","iopub.status.idle":"2025-05-17T18:50:59.250526Z","shell.execute_reply.started":"2025-05-17T18:50:59.246368Z","shell.execute_reply":"2025-05-17T18:50:59.249719Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 3) load Qwen2‐VL‐2B‐Instruct baseline\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n\n# 2) Plug in your LoRA adapter and merge for inference\nmodel_ft = PeftModel.from_pretrained(\n    model,\n    \"/kaggle/input/16-may-qwen-2-finetuned/lora_adapters_fp16\",  \n    torch_dtype=torch.float16\n)\nmodel_ft = model_ft.merge_and_unload()  # fuse adapter into base\nmodel_ft.eval()\n\n# 3) Reload your processor (same as baseline)\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    use_fast=True,\n    trust_remote_code=True\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:50:59.251592Z","iopub.execute_input":"2025-05-17T18:50:59.251849Z","iopub.status.idle":"2025-05-17T18:51:38.751690Z","shell.execute_reply.started":"2025-05-17T18:50:59.251830Z","shell.execute_reply":"2025-05-17T18:51:38.751043Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f3024b37e440cabf1eecfd2b4eace7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec651bb7e49e4ea98f07e613562a8daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9215f907a44383a6946cbcd87e65a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e1fcded5754566b01b29ea30bf95e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fca3a3c880f493b81503468be2bcec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6908dba2ed245fc87fcc6d6f8b54b04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ce0f7bf51841e0b6bf0a976ece5ab4"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:396: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n```python\nfrom transformers import AutoModelForCausalLM\n\n# Load original tied model\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n\n# Set the randomly initialized lm_head to the previously tied embeddings\nmodel.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n\n# Save the untied model\nuntied_model_dir = \"dir/for/untied/model\"\nmodel.save_pretrained(untied_model_dir)\nmodel.config.save_pretrained(untied_model_dir)\n\n# Now use the original model but in untied format\nmodel = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n```\n\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb95642279d444ea84e4fa40ed8851f6"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076ceda55005464e9915a5454e3a788c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945b7d76d68648d396c987512f92bfa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632d4a1645f446f2992a04666ae58550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80297eb498c44961a9baa2ca1525bcb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2355ae42652f432c9e043cb4bb431fdb"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class PadTo256:\n    def __init__(self, fill=255):\n        # fill=255 gives white; for RGB you could also do fill=(255,255,255)\n        self.fill = fill\n\n    def __call__(self, img: Image.Image) -> Image.Image:\n        # 1) make a copy so we don't clobber the original\n        img = img.copy()\n\n        # 2) thumbnail will scale **in-place** so that both width & height <= (256,256)\n        #    preserving aspect ratio\n        img.thumbnail((256, 256), Image.BILINEAR)\n\n        w, h = img.size\n        # 3) compute how much padding is needed on each side\n        pad_left   = (256 - w) // 2\n        pad_top    = (256 - h) // 2\n        pad_right  = 256 - w - pad_left\n        pad_bottom = 256 - h - pad_top\n\n        # 4) expand with white border\n        return ImageOps.expand(img, border=(pad_left, pad_top, pad_right, pad_bottom),\n                               fill=self.fill)\n\n\n# instantiate once:\nRESIZE = PadTo256(fill=(255,255,255))   # white pad\n# in collate_fn, just:","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:51:38.752486Z","iopub.execute_input":"2025-05-17T18:51:38.752698Z","iopub.status.idle":"2025-05-17T18:51:38.758816Z","shell.execute_reply.started":"2025-05-17T18:51:38.752681Z","shell.execute_reply":"2025-05-17T18:51:38.758235Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import math\n\n# 1) parameters\nbatch_size  = 8\npreds, refs = [], []\nimage_root  = \"/kaggle/input/test-data-dataset/images\"\nn_samples   = len(df)\nn_batches   = math.ceil(n_samples / batch_size)\n\n# 2) batched inference\nstart_time = time.time()\nfor batch_start in tqdm(range(0, n_samples, batch_size),\n                        total=n_batches,\n                        desc=\"Qwen2-VL Batched Inference\"):\n    batch_df = df.iloc[batch_start : batch_start + batch_size]\n\n    # 2a) build list of “chat” messages for the batch\n    batch_messages = []\n    for _, row in batch_df.iterrows():\n        img = Image.open(os.path.join(image_root, row[\"path\"])).convert(\"RGB\")\n        img = RESIZE(img)\n        batch_messages.append([\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": img},\n                {\"type\": \"text\",  \"text\": f\"Answer in exactly one word: {row['question']}\"}\n            ]}\n        ])\n\n    # 2b) apply the chat template in batch\n    batch_texts = [\n        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n        for msg in batch_messages\n    ]\n\n    # 2c) extract vision inputs for the whole batch\n    image_inputs, video_inputs = process_vision_info(batch_messages)\n\n    # 2d) tokenize the entire batch at once\n    inputs = processor(\n        text=batch_texts,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    # 2e) generate in one shot\n    with torch.inference_mode():\n        generated = model_ft.generate(**inputs, max_new_tokens=32)\n\n    # 2f) decode each example, trimming off the prompt\n    for in_ids, out_ids in zip(inputs.input_ids, generated):\n        trimmed_ids = out_ids[len(in_ids):]\n        text_out   = processor.batch_decode(\n            [trimmed_ids],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=False\n        )[0].strip()\n        preds.append(text_out)\n\n    # 2g) collect references\n    refs.extend(batch_df[\"answer\"].astype(str).str.strip().tolist())\n\nend_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T18:53:10.573977Z","iopub.execute_input":"2025-05-17T18:53:10.574282Z","iopub.status.idle":"2025-05-17T19:03:29.745414Z","shell.execute_reply.started":"2025-05-17T18:53:10.574261Z","shell.execute_reply":"2025-05-17T19:03:29.744245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Qwen2-VL Batched Inference:   0%|          | 0/750 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a322c4d5dcc64930a684d6449a20e528"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# # 4) inference loop with timing\n# preds, refs = [], []\n# image_root  = \"/kaggle/input/test-data-dataset/images\"\n\n# start_time = time.time()\n# for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Qwen2-VL Inference\"):\n#     # load & preprocess image\n#     img_path = os.path.join(image_root, row[\"path\"])\n#     image    = Image.open(img_path).convert(\"RGB\").resize((256, 256))\n    \n#     # build the chat‐style message\n#     messages = [\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\"type\": \"image\", \"image\": image},\n#                 {\"type\": \"text\",  \"text\":  row[\"question\"]}\n#             ],\n#         }\n#     ]\n#     # apply template\n#     text = processor.apply_chat_template(\n#         messages, tokenize=False, add_generation_prompt=True\n#     )\n#     # extract vision inputs\n#     image_inputs, video_inputs = process_vision_info(messages)\n#     # tokenize & move to device\n#     inputs = processor(\n#         text=[text],\n#         images=image_inputs,\n#         videos=video_inputs,\n#         padding=True,\n#         return_tensors=\"pt\",\n#     ).to(device)\n\n#     # generate\n#     with torch.inference_mode():\n#         generated_ids = model.generate(**inputs, max_new_tokens=32)\n\n#     # trim off prompt tokens\n#     trimmed_ids = [\n#         out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n#     ]\n#     output_text = processor.batch_decode(\n#         trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n#     )\n#     pred = output_text[0].strip()\n\n#     preds.append(pred)\n#     refs.append(str(row[\"answer\"]).strip())\n\n# end_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:03:29.747217Z","iopub.execute_input":"2025-05-17T19:03:29.747505Z","iopub.status.idle":"2025-05-17T19:03:29.752761Z","shell.execute_reply.started":"2025-05-17T19:03:29.747471Z","shell.execute_reply":"2025-05-17T19:03:29.751774Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 5) compute metrics\ntotal_time    = end_time - start_time\navg_time_ms   = total_time / len(preds) * 1000\n\n# Exact‐Match Accuracy\nexact_match = sum(p.lower()==r.lower() for p,r in zip(preds, refs)) / len(preds)\n\n# BERTScore F1 (all preds)\nP_all, R_all, F1_all = score(preds, refs, lang=\"en\", rescale_with_baseline=True)\nbert_f1_all = F1_all.mean().item()\n\n# One‐word analysis\nis_one_word  = [((\" \" not in p) and p!= \"\") for p in preds]\npct_one_word = sum(is_one_word) / len(preds) * 100\npreds_1w     = [p for p, ok in zip(preds, is_one_word) if ok]\nrefs_1w      = [r for r, ok in zip(refs,  is_one_word) if ok]\n\nif preds_1w:\n    _, _, F1_1w = score(preds_1w, refs_1w, lang=\"en\", rescale_with_baseline=True)\n    bert_f1_1w = F1_1w.mean().item()\nelse:\n    bert_f1_1w = float(\"nan\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:03:29.753893Z","iopub.execute_input":"2025-05-17T19:03:29.754173Z","iopub.status.idle":"2025-05-17T19:03:48.223499Z","shell.execute_reply.started":"2025-05-17T19:03:29.754149Z","shell.execute_reply":"2025-05-17T19:03:48.222391Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d06b48f06724b32a09604cab41cf58f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58235a29bee945ffa13c339145f7ac0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37db9747245b4b078e45701c55723c0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f1cac77049474fb2f0f0ecd3bf17c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80209306f86e45e6a052e2c6d793375c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b036bee2a10246f5ae302e988eb791e6"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 6) report\nprint(f\"Total inference time (1000 samples): {total_time:.1f}s\")\nprint(f\"Average per sample:                {avg_time_ms:.1f}ms\\n\")\nprint(f\"Exact-Match Accuracy:              {exact_match:.2f}\")\nprint(f\"BERTScore F1 (all preds):          {bert_f1_all:.2f}\\n\")\nprint(f\"% One-Word Predictions:            {pct_one_word:.2f}%\")\nprint(f\"BERTScore F1 (one-word only):      {bert_f1_1w:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:03:48.225538Z","iopub.execute_input":"2025-05-17T19:03:48.225832Z","iopub.status.idle":"2025-05-17T19:03:48.233086Z","shell.execute_reply.started":"2025-05-17T19:03:48.225807Z","shell.execute_reply":"2025-05-17T19:03:48.232088Z"}},"outputs":[{"name":"stdout","text":"Total inference time (1000 samples): 619.2s\nAverage per sample:                103.3ms\n\nExact-Match Accuracy:              0.66\nBERTScore F1 (all preds):          0.84\n\n% One-Word Predictions:            96.28%\nBERTScore F1 (one-word only):      0.87\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}